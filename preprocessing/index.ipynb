{"cells":[{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":848,"status":"ok","timestamp":1681351422685,"user":{"displayName":"Darshana Colab","userId":"07109758458813485529"},"user_tz":-330},"id":"hJBEw75MVK78"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import pickle\n","import bz2"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["path = \"../capture24\""]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1681351444680,"user":{"displayName":"Darshana Colab","userId":"07109758458813485529"},"user_tz":-330},"id":"GzvaeMZUVK8A"},"outputs":[],"source":["def create_windows_np(data, window_size, stride):\n","    num_samples, num_channels = data.shape\n","    num_windows = (num_samples - window_size) // stride + 1\n","\n","    shape = (num_windows, window_size, num_channels)\n","    strides = (data.strides[0] * stride, data.strides[0], data.strides[1])\n","\n","    windows = np.lib.stride_tricks.as_strided(data, shape=shape, strides=strides)\n","\n","    # transpose the windows array to the desired shape\n","    windows = np.transpose(windows, axes=(0, 2, 1))\n","\n","    return windows"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def create_sample_datasets(lower_bound, upper_bound, dataset_name, path):\n","    df_all = pd.DataFrame({'x': [], 'y': [], 'z': [], 'user_id': []})  # add a 'user_id' column to the DataFrame\n","    for i in range(lower_bound, upper_bound):\n","\n","        user_id = f'P{i+1:03d}'  # set the user ID for this iteration\n","\n","        df = pd.read_csv(f'{path}/{user_id}.csv.gz', compression='gzip', low_memory=False)\n","        df = df.drop(columns=['annotation'])\n","        \n","        # convert the 'time' column to a datetime object\n","        df['time'] = pd.to_datetime(df['time'], origin='unix')\n","        \n","        # set the 'time' column as the DataFrame index\n","        df.set_index('time', inplace=True)\n","        \n","        # resample the DataFrame from 100 Hz to 50 Hz\n","        df = df.resample('20ms').mean()\n","        \n","        df = df.reset_index().reset_index(drop=True)\n","        df['user_id'] = i+1  # set the user ID for this DataFrame\n","        df_all = pd.concat([df_all, df.loc[:df.shape[0] - df.shape[0] % 100 - 1]])\n","        print(f'{user_id}: {df.shape[0]}')\n","        del df\n","\n","    # reset the index of the df_all DataFrame\n","    df_all = df_all.reset_index(drop=True)\n","\n","    # save the df_all DataFrame to a Feather file\n","    df_all.to_feather(f'{path}/{dataset_name}.feather')\n","    del df_all"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["P021: 4770001\n","P022: 5340001\n","P023: 5040001\n","P024: 5040001\n"]}],"source":["create_sample_datasets(0, 20, 'training', path)\n","create_sample_datasets(20, 22, 'validation', path)\n","create_sample_datasets(22, 24, 'test', path)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","\n","df_train = pd.read_feather(f'{path}/training.feather')\n","\n","std_scaler = StandardScaler()\n","std_scaler.fit(df_train[['x', 'y', 'z']])\n","\n","df_train[['x', 'y', 'z']] = std_scaler.transform(df_train[['x', 'y', 'z']])\n","df_train.to_feather(f'{path}/training.feather')\n","del df_train\n","\n","df_validation = pd.read_feather(f'{path}/validation.feather')\n","df_test = pd.read_feather(f'{path}/test.feather')\n","\n","df_validation[['x', 'y', 'z']] = std_scaler.transform(df_validation[['x', 'y', 'z']])\n","df_validation.to_feather(f'{path}/validation.feather')\n","df_test[['x', 'y', 'z']] = std_scaler.transform(df_test[['x', 'y', 'z']])\n","df_test.to_feather(f'{path}/test.feather')\n","\n","del df_validation, df_test"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>x</th>\n","      <th>y</th>\n","      <th>z</th>\n","      <th>user_id</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-0.414665</td>\n","      <td>-1.035181</td>\n","      <td>0.823605</td>\n","      <td>1.0</td>\n","      <td>2016-11-13 02:18:00.000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-0.414665</td>\n","      <td>-1.035181</td>\n","      <td>0.823605</td>\n","      <td>1.0</td>\n","      <td>2016-11-13 02:18:00.020</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-0.414665</td>\n","      <td>-1.035181</td>\n","      <td>0.823605</td>\n","      <td>1.0</td>\n","      <td>2016-11-13 02:18:00.040</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-0.414665</td>\n","      <td>-1.050719</td>\n","      <td>0.823605</td>\n","      <td>1.0</td>\n","      <td>2016-11-13 02:18:00.060</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-0.414665</td>\n","      <td>-1.050719</td>\n","      <td>0.823605</td>\n","      <td>1.0</td>\n","      <td>2016-11-13 02:18:00.080</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          x         y         z  user_id                    time\n","0 -0.414665 -1.035181  0.823605      1.0 2016-11-13 02:18:00.000\n","1 -0.414665 -1.035181  0.823605      1.0 2016-11-13 02:18:00.020\n","2 -0.414665 -1.035181  0.823605      1.0 2016-11-13 02:18:00.040\n","3 -0.414665 -1.050719  0.823605      1.0 2016-11-13 02:18:00.060\n","4 -0.414665 -1.050719  0.823605      1.0 2016-11-13 02:18:00.080"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["df_train = pd.read_feather(f'{path}/training.feather')\n","df_train.head()"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def noise_transform_vectorized(X, sigma=0.05):\n","    \"\"\"\n","    Adding random Gaussian noise with mean 0\n","    \"\"\"\n","    noise = np.random.normal(loc=0, scale=sigma, size=X.shape)\n","    return X + noise\n","\n","def scaling_transform_vectorized(X, sigma=0.1):\n","    \"\"\"\n","    Scaling by a random factor\n","    \"\"\"\n","    scaling_factor = np.random.normal(loc=1.0, scale=sigma, size=(X.shape[0], 1, X.shape[2]))\n","    return X * scaling_factor\n","\n","def negate_transform_vectorized(X):\n","    \"\"\"\n","    Inverting the signals\n","    \"\"\"\n","    return X * -1\n","\n","def time_flip_transform_vectorized(X):\n","    \"\"\"\n","    Reversing the direction of time\n","    \"\"\"\n","    return X[:, ::-1, :]"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# Define the list of transformations to be applied\n","transformations = [\n","    lambda x: noise_transform_vectorized(x), \n","    lambda x: scaling_transform_vectorized(x),\n","    lambda x: negate_transform_vectorized(x),\n","    lambda x: time_flip_transform_vectorized(x),\n","    ]"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":517,"status":"ok","timestamp":1681351609657,"user":{"displayName":"Darshana Colab","userId":"07109758458813485529"},"user_tz":-330},"id":"NfV-xrXJVK8C"},"outputs":[{"name":"stdout","output_type":"stream","text":["(500995, 4, 100)\n","(500995,)\n"]}],"source":["output_path = '/media/darshana/Software/dataset'\n","user_dataset = {}\n","\n","df = pd.read_feather(f'{path}/training.feather', columns=['x', 'y', 'z', 'user_id'])\n","\n","for i in range(1):\n","    \n","    user_data = create_windows_np(df.loc[df['user_id'] == i+1].values.astype(np.float32), 100, 50)\n","\n","    # Get the number of windows and window size for the user's data\n","    num_windows, window_size, num_channels = user_data.shape\n","\n","    # Apply the transformations to the user's data\n","    transformed_data = np.concatenate([transform_fn(user_data) for transform_fn in transformations], axis=0)\n","    transformed_data = np.concatenate([transformed_data, user_data], axis=0)\n","    transformed_data = np.array(transformed_data)\n","\n","    # Create the labels for the transformed data\n","    transformed_labels = np.array([False for _ in range(4)])\n","    transformed_labels = np.append(transformed_labels, True)\n","    transformed_labels = np.repeat(transformed_labels, num_windows)\n","\n","    print(transformed_data.shape)\n","    print(transformed_labels.shape)\n","\n","    transformed_user_data = {\n","        'X': transformed_data,\n","        'y': transformed_labels\n","    }\n","\n","    del transformed_data,  transformed_labels\n","\n","    with bz2.BZ2File(f'{output_path}/P{i+1:03d}.pbz2', 'wb') as f:\n","        pickle.dump(transformed_user_data, f)\n","\n","    del transformed_user_data\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["p001file = open(f'/media/darshana/Software/dataset/P001.obj', 'rb')\n","p001Dataset = pickle.load(p001file)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'X': array([[[-0.55305357, -0.51706906,  0.69314428],\n","        [-0.50472098, -0.56538143,  0.65664381],\n","        [-0.44077426, -0.53410961,  0.59784726],\n","        ...,\n","        [-0.47601569, -0.48092514,  0.68703596],\n","        [-0.46727538, -0.53516123,  0.74507716],\n","        [-0.43284447, -0.50125229,  0.6164534 ]],\n","\n","       [[-0.47427553, -0.53181389,  0.56354433],\n","        [-0.43548332, -0.6421605 ,  0.69263543],\n","        [-0.45257494, -0.47719815,  0.6296225 ],\n","        ...,\n","        [-0.49897984, -0.51259347,  0.61318453],\n","        [-0.42932175, -0.53151222,  0.64826092],\n","        [-0.47932165, -0.53827612,  0.61613918]],\n","\n","       [[-0.43956137, -0.50406777,  0.6121599 ],\n","        [-0.44457909, -0.50826508,  0.64317895],\n","        [-0.43738584, -0.51463917,  0.68245852],\n","        ...,\n","        [-0.42746603, -0.51584196,  0.71400161],\n","        [-0.48124537, -0.57680277,  0.71200579],\n","        [-0.42053982, -0.44312963,  0.61433704]],\n","\n","       ...,\n","\n","       [[ 0.0494156 , -0.79784578,  0.56569976],\n","        [ 0.03377215, -0.79784578,  0.56569976],\n","        [ 0.03377215, -0.79784578,  0.56569976],\n","        ...,\n","        [ 0.0494156 , -0.78228456,  0.56569976],\n","        [ 0.03377215, -0.78228456,  0.56569976],\n","        [ 0.03377215, -0.78228456,  0.56569976]],\n","\n","       [[ 0.0494156 , -0.79784578,  0.56569976],\n","        [ 0.03377215, -0.78228456,  0.56569976],\n","        [ 0.0494156 , -0.79784578,  0.55030459],\n","        ...,\n","        [ 0.03377215, -0.78228456,  0.56569976],\n","        [ 0.03377215, -0.79784578,  0.56569976],\n","        [ 0.0494156 , -0.78228456,  0.55030459]],\n","\n","       [[ 0.0494156 , -0.79784578,  0.56569976],\n","        [ 0.03377215, -0.78228456,  0.56569976],\n","        [ 0.03377215, -0.78228456,  0.56569976],\n","        ...,\n","        [ 0.0494156 , -0.78228456,  0.56569976],\n","        [ 0.0494156 , -0.78228456,  0.56569976],\n","        [ 0.0494156 , -0.78228456,  0.56569976]]]), 'y': array(['Noise', 'Noise', 'Noise', ..., 'Original', 'Original', 'Original'],\n","      dtype='<U9')}\n"]}],"source":["print(p001Dataset)\n","del p001Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = pd.read_csv(f'{path}/P{i+1:03d}.csv.gz', compression='gzip', low_memory=False)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":0}
