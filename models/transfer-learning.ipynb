{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskTPN(nn.Module):\n",
    "    def __init__(self, num_tasks=4, num_channels=3):\n",
    "        super(MultiTaskTPN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(num_channels, 32, kernel_size=24, stride=1)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=16, stride=1)\n",
    "        self.conv3 = nn.Conv1d(64, 96, kernel_size=8, stride=1)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "        self.task_heads = nn.ModuleList([nn.Sequential(\n",
    "            nn.Linear(96, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        ) for _ in range(num_tasks)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.dropout(nn.functional.relu(self.conv2(x)))\n",
    "        x = self.dropout(nn.functional.relu(self.conv3(x)))\n",
    "        x = nn.functional.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "\n",
    "        logits = [task_head(x).view(-1, 1) for task_head in self.task_heads]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnergyExpenditureTPN(MultiTaskTPN):\n",
    "    def __init__(self, num_channels=3):\n",
    "        super(EnergyExpenditureTPN, self).__init__(num_tasks=1, num_channels=num_channels)\n",
    "        \n",
    "        # Remove the dropout layers from task_heads\n",
    "        self.task_heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(96, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, 1)\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        # Freeze the feature extraction layers\n",
    "        for param in self.conv1.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.conv2.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.conv3.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.dropout(nn.functional.relu(self.conv2(x)))\n",
    "        x = self.dropout(nn.functional.relu(self.conv3(x)))\n",
    "        x = nn.functional.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "\n",
    "        output = self.task_heads[0](x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windows_np(data, window_size, stride):\n",
    "    num_samples, num_channels = data.shape\n",
    "    num_windows = (num_samples - window_size) // stride + 1\n",
    "\n",
    "    shape = (num_windows, window_size, num_channels)\n",
    "    strides = (data.strides[0] * stride, data.strides[0], data.strides[1])\n",
    "\n",
    "    windows = np.lib.stride_tricks.as_strided(data, shape=shape, strides=strides)\n",
    "\n",
    "    # transpose the windows array to the desired shape\n",
    "    windows = np.transpose(windows, axes=(0, 2, 1))\n",
    "\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiTaskTPN(\n",
       "  (conv1): Conv1d(3, 32, kernel_size=(24,), stride=(1,))\n",
       "  (conv2): Conv1d(32, 64, kernel_size=(16,), stride=(1,))\n",
       "  (conv3): Conv1d(64, 96, kernel_size=(8,), stride=(1,))\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (task_heads): ModuleList(\n",
       "    (0-3): 4 x Sequential(\n",
       "      (0): Linear(in_features=96, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "      (4): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the model\n",
    "loaded_model = MultiTaskTPN().to(device)\n",
    "\n",
    "# Load the saved state dictionary\n",
    "loaded_model.load_state_dict(torch.load('multi_task_tpn.pth'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(i, df, batch_size):\n",
    "    user_data = {\n",
    "        'X': [],\n",
    "        'y': []\n",
    "    }\n",
    "\n",
    "    # Get the data for the current user\n",
    "    user_data['X'] = create_windows_np(df.loc[df['user_id'] == i+1, ['x', 'y', 'z']].values.astype(np.float32), 100, 50)\n",
    "\n",
    "    # Get the labels for the current user\n",
    "    y = df.loc[df['user_id'] == i+1, 'met_value_mean_values'].values.astype(np.float32)\n",
    "\n",
    "    # Create the 50% overlapping labels windows size of 100 without create_windows_np or for loop\n",
    "    user_data['y'] = np.array([y[i:i+100].mean() for i in range(0, len(y), 50)])[:-2]\n",
    "\n",
    "    # Get the missing value indexes in the user_data['y']\n",
    "    missing_value_indexes = np.argwhere(np.isnan(user_data['y']))\n",
    "\n",
    "    # Remove the missing values from the user_data['y'] and user_data['X']\n",
    "    user_data['y'] = np.delete(user_data['y'], missing_value_indexes)\n",
    "    user_data['X'] = np.delete(user_data['X'], missing_value_indexes, axis=0)\n",
    "\n",
    "    # Create a DataLoader to iterate over the test data in batches\n",
    "    dataset = TensorDataset(torch.tensor(user_data['X'], dtype=torch.float32), \n",
    "                                torch.tensor(user_data['y'], dtype=torch.float32))\n",
    "    \n",
    "    # Create a DataLoader to iterate over the test data in batches\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/250, Training Loss: 1.0398294925689697, Validation Loss: 1.4089439163605373\n",
      "Epoch: 2/250, Training Loss: 0.8577562570571899, Validation Loss: 1.36596084634463\n",
      "Epoch: 3/250, Training Loss: 0.9803221821784973, Validation Loss: 1.3152619749307632\n",
      "Epoch: 4/250, Training Loss: 0.8033965229988098, Validation Loss: 1.2834422985712688\n",
      "Epoch: 5/250, Training Loss: 0.9088019132614136, Validation Loss: 1.3180226335922878\n",
      "Epoch: 6/250, Training Loss: 0.799903154373169, Validation Loss: 1.2665912260611851\n",
      "Epoch: 7/250, Training Loss: 1.0082985162734985, Validation Loss: 1.3438113331794739\n",
      "Epoch: 8/250, Training Loss: 0.989181399345398, Validation Loss: 1.2643354485432308\n",
      "Epoch: 9/250, Training Loss: 1.092362880706787, Validation Loss: 1.3505771656831105\n",
      "Epoch: 10/250, Training Loss: 0.904577374458313, Validation Loss: 1.265009770790736\n",
      "Epoch: 11/250, Training Loss: 0.8848182559013367, Validation Loss: 1.3689132531483967\n",
      "Epoch: 12/250, Training Loss: 0.8348012566566467, Validation Loss: 1.3185224731763203\n",
      "Epoch: 13/250, Training Loss: 0.9126166701316833, Validation Loss: 1.2952357232570648\n",
      "Epoch: 14/250, Training Loss: 0.9222315549850464, Validation Loss: 1.335654338200887\n",
      "Epoch: 15/250, Training Loss: 0.9315428137779236, Validation Loss: 1.274355173110962\n",
      "Epoch: 16/250, Training Loss: 0.7804080247879028, Validation Loss: 1.3090895116329193\n",
      "Epoch: 17/250, Training Loss: 1.0483440160751343, Validation Loss: 1.3625101447105408\n",
      "Epoch: 18/250, Training Loss: 0.8695272207260132, Validation Loss: 1.3792460560798645\n",
      "Epoch: 19/250, Training Loss: 1.054738163948059, Validation Loss: 1.3051765263080597\n",
      "Epoch: 20/250, Training Loss: 0.7919153571128845, Validation Loss: 1.2826242198546727\n",
      "Epoch: 21/250, Training Loss: 0.8473027944564819, Validation Loss: 1.287142405907313\n",
      "Epoch: 22/250, Training Loss: 0.9033491611480713, Validation Loss: 1.2417499174674351\n",
      "Epoch: 23/250, Training Loss: 0.8820836544036865, Validation Loss: 1.2463608632485073\n",
      "Epoch: 24/250, Training Loss: 0.8230170011520386, Validation Loss: 1.2453028559684753\n",
      "Epoch: 25/250, Training Loss: 0.8960552215576172, Validation Loss: 1.214600572983424\n",
      "Epoch: 26/250, Training Loss: 0.8094899654388428, Validation Loss: 1.231274574995041\n",
      "Epoch: 27/250, Training Loss: 0.8469655513763428, Validation Loss: 1.250233918428421\n",
      "Epoch: 28/250, Training Loss: 0.8489364981651306, Validation Loss: 1.251700644691785\n",
      "Epoch: 29/250, Training Loss: 0.8190481662750244, Validation Loss: 1.23985655605793\n",
      "Epoch: 30/250, Training Loss: 0.788441002368927, Validation Loss: 1.2014009257157643\n",
      "Epoch: 31/250, Training Loss: 0.7040097117424011, Validation Loss: 1.2087616076072056\n",
      "Epoch: 32/250, Training Loss: 0.7855115532875061, Validation Loss: 1.2320790688196819\n",
      "Epoch: 33/250, Training Loss: 0.9176784157752991, Validation Loss: 1.3292014102141063\n",
      "Epoch: 34/250, Training Loss: 0.8331937789916992, Validation Loss: 1.2580073724190395\n",
      "Epoch: 35/250, Training Loss: 0.7522085309028625, Validation Loss: 1.169211412469546\n",
      "Epoch: 36/250, Training Loss: 0.7949417233467102, Validation Loss: 1.2775047818819683\n",
      "Epoch: 37/250, Training Loss: 0.9480855464935303, Validation Loss: 1.269192303220431\n",
      "Epoch: 38/250, Training Loss: 0.7346761226654053, Validation Loss: 1.207663709918658\n",
      "Epoch: 39/250, Training Loss: 0.8778427839279175, Validation Loss: 1.2153954009215038\n",
      "Epoch: 40/250, Training Loss: 0.789444625377655, Validation Loss: 1.2132927825053532\n",
      "Epoch: 41/250, Training Loss: 0.7657783031463623, Validation Loss: 1.1864922940731049\n",
      "Epoch: 42/250, Training Loss: 0.9241365790367126, Validation Loss: 1.268210972348849\n",
      "Epoch: 43/250, Training Loss: 0.998399555683136, Validation Loss: 1.3422620395819347\n",
      "Epoch: 44/250, Training Loss: 0.8191396594047546, Validation Loss: 1.1854421546061833\n",
      "Epoch: 45/250, Training Loss: 0.8804428577423096, Validation Loss: 1.215843473871549\n",
      "Epoch: 46/250, Training Loss: 0.7448061108589172, Validation Loss: 1.2159380316734314\n",
      "Epoch: 47/250, Training Loss: 0.7880126237869263, Validation Loss: 1.2168147017558415\n",
      "Epoch: 48/250, Training Loss: 0.8338375091552734, Validation Loss: 1.222618728876114\n",
      "Epoch: 49/250, Training Loss: 0.8183097243309021, Validation Loss: 1.2126365999380748\n",
      "Epoch: 50/250, Training Loss: 0.8335842490196228, Validation Loss: 1.2437881579001744\n",
      "Epoch: 51/250, Training Loss: 0.8577609658241272, Validation Loss: 1.2133147418498993\n",
      "Epoch: 52/250, Training Loss: 0.8252416253089905, Validation Loss: 1.2173628956079483\n",
      "Epoch: 53/250, Training Loss: 0.8710672855377197, Validation Loss: 1.1794559061527252\n",
      "Epoch: 54/250, Training Loss: 0.8147992491722107, Validation Loss: 1.1698679625988007\n",
      "Epoch: 55/250, Training Loss: 0.8339323997497559, Validation Loss: 1.2362068643172581\n",
      "Epoch: 56/250, Training Loss: 0.892122745513916, Validation Loss: 1.2225031505028408\n",
      "Epoch: 57/250, Training Loss: 0.8480497598648071, Validation Loss: 1.211138019959132\n",
      "Epoch: 58/250, Training Loss: 0.7922723889350891, Validation Loss: 1.158558304111163\n",
      "Epoch: 59/250, Training Loss: 0.7718201875686646, Validation Loss: 1.1528064111868541\n",
      "Epoch: 60/250, Training Loss: 0.9447759389877319, Validation Loss: 1.2012223700682323\n",
      "Epoch: 61/250, Training Loss: 0.6754141449928284, Validation Loss: 1.1593451996644337\n",
      "Epoch: 62/250, Training Loss: 0.7181630730628967, Validation Loss: 1.1575590868790944\n",
      "Epoch: 63/250, Training Loss: 0.7342036366462708, Validation Loss: 1.2272272855043411\n",
      "Epoch: 64/250, Training Loss: 0.8029286861419678, Validation Loss: 1.2016339649756749\n",
      "Epoch: 65/250, Training Loss: 0.793542206287384, Validation Loss: 1.1570706069469452\n",
      "Epoch: 66/250, Training Loss: 0.9272208213806152, Validation Loss: 1.2059214413166046\n",
      "Epoch: 67/250, Training Loss: 0.8688594698905945, Validation Loss: 1.2281755059957504\n",
      "Epoch: 68/250, Training Loss: 0.8467675447463989, Validation Loss: 1.2598021924495697\n",
      "Epoch: 69/250, Training Loss: 0.8852397799491882, Validation Loss: 1.1853440304597218\n",
      "Epoch: 70/250, Training Loss: 0.8007398843765259, Validation Loss: 1.2401387244462967\n",
      "Epoch: 71/250, Training Loss: 0.849344789981842, Validation Loss: 1.1826098610957463\n",
      "Epoch: 72/250, Training Loss: 0.8630443811416626, Validation Loss: 1.2482048521439235\n",
      "Epoch: 73/250, Training Loss: 0.7448068261146545, Validation Loss: 1.1573520849148433\n",
      "Epoch: 74/250, Training Loss: 0.9436474442481995, Validation Loss: 1.1687317689259846\n",
      "Epoch: 75/250, Training Loss: 0.8668516278266907, Validation Loss: 1.220072329044342\n",
      "Epoch: 76/250, Training Loss: 0.8358572125434875, Validation Loss: 1.2022587358951569\n",
      "Epoch: 77/250, Training Loss: 0.8665306568145752, Validation Loss: 1.2383652528127034\n",
      "Epoch: 78/250, Training Loss: 0.8074145913124084, Validation Loss: 1.215523213148117\n",
      "Epoch: 79/250, Training Loss: 0.7951614260673523, Validation Loss: 1.1725621620814006\n",
      "Epoch: 80/250, Training Loss: 0.8194295763969421, Validation Loss: 1.209261601169904\n",
      "Epoch: 81/250, Training Loss: 0.9510693550109863, Validation Loss: 1.2630061656236649\n",
      "Epoch: 82/250, Training Loss: 0.8766115307807922, Validation Loss: 1.2015615204970043\n",
      "Epoch: 83/250, Training Loss: 0.8313284516334534, Validation Loss: 1.1890467753012974\n",
      "Epoch: 84/250, Training Loss: 0.7984724044799805, Validation Loss: 1.1963433623313904\n",
      "Epoch: 85/250, Training Loss: 0.8493738174438477, Validation Loss: 1.2615889211495717\n",
      "Epoch: 86/250, Training Loss: 0.7871868014335632, Validation Loss: 1.199049989382426\n",
      "Epoch: 87/250, Training Loss: 0.7678180932998657, Validation Loss: 1.1915735254685085\n",
      "Epoch: 88/250, Training Loss: 0.9650352597236633, Validation Loss: 1.26727028687795\n",
      "Epoch: 89/250, Training Loss: 0.9355511665344238, Validation Loss: 1.2771804680426915\n",
      "Epoch: 90/250, Training Loss: 0.9491739869117737, Validation Loss: 1.2376760790745418\n",
      "Epoch: 91/250, Training Loss: 0.7351463437080383, Validation Loss: 1.139979287981987\n",
      "Epoch: 92/250, Training Loss: 0.8456091284751892, Validation Loss: 1.243603656689326\n",
      "Epoch: 93/250, Training Loss: 0.8415470719337463, Validation Loss: 1.2511984060208003\n",
      "Epoch: 94/250, Training Loss: 0.9753317832946777, Validation Loss: 1.2904500861962636\n",
      "Epoch: 95/250, Training Loss: 0.8050448894500732, Validation Loss: 1.1624836325645447\n",
      "Epoch: 96/250, Training Loss: 0.9511586427688599, Validation Loss: 1.2520761688550313\n",
      "Epoch: 97/250, Training Loss: 0.8611635565757751, Validation Loss: 1.2652837882439296\n",
      "Epoch: 98/250, Training Loss: 0.9529415965080261, Validation Loss: 1.2743216852347057\n",
      "Epoch: 99/250, Training Loss: 0.864071249961853, Validation Loss: 1.2224574933449428\n",
      "Epoch: 100/250, Training Loss: 0.9119116067886353, Validation Loss: 1.2471164812644322\n",
      "Epoch: 101/250, Training Loss: 0.7757458686828613, Validation Loss: 1.2145178318023682\n",
      "Epoch: 102/250, Training Loss: 0.848726212978363, Validation Loss: 1.2164343545834224\n",
      "Epoch: 103/250, Training Loss: 0.7929045557975769, Validation Loss: 1.2553563763697941\n",
      "Epoch: 104/250, Training Loss: 0.8956903219223022, Validation Loss: 1.2601836323738098\n",
      "Epoch: 105/250, Training Loss: 0.820770800113678, Validation Loss: 1.2291313360134761\n",
      "Epoch: 106/250, Training Loss: 0.8826239705085754, Validation Loss: 1.2336838245391846\n",
      "Epoch: 107/250, Training Loss: 0.7966766953468323, Validation Loss: 1.2169737716515858\n",
      "Epoch: 108/250, Training Loss: 0.783650279045105, Validation Loss: 1.2652134448289871\n",
      "Epoch: 109/250, Training Loss: 0.7327607870101929, Validation Loss: 1.2129454414049785\n",
      "Epoch: 110/250, Training Loss: 0.876345694065094, Validation Loss: 1.3299291481574376\n",
      "Epoch: 111/250, Training Loss: 0.9187645316123962, Validation Loss: 1.2609808693329494\n",
      "Epoch: 112/250, Training Loss: 0.9521530866622925, Validation Loss: 1.2389333595832188\n",
      "Epoch: 113/250, Training Loss: 0.7899090051651001, Validation Loss: 1.258213922381401\n",
      "Epoch: 114/250, Training Loss: 0.9764491319656372, Validation Loss: 1.337593361735344\n",
      "Epoch: 115/250, Training Loss: 0.841261088848114, Validation Loss: 1.2856187224388123\n",
      "Epoch: 116/250, Training Loss: 0.8997659683227539, Validation Loss: 1.243704338868459\n",
      "Epoch: 117/250, Training Loss: 0.9016764760017395, Validation Loss: 1.2979510525862377\n",
      "Epoch: 118/250, Training Loss: 0.8295919299125671, Validation Loss: 1.2754482279221218\n",
      "Epoch: 119/250, Training Loss: 0.849880576133728, Validation Loss: 1.2049235204855602\n",
      "Epoch: 120/250, Training Loss: 0.9152908325195312, Validation Loss: 1.3188927918672562\n",
      "Epoch: 121/250, Training Loss: 0.9640862345695496, Validation Loss: 1.3003176997105281\n",
      "Epoch: 122/250, Training Loss: 0.7915372848510742, Validation Loss: 1.2182031671206157\n",
      "Epoch: 123/250, Training Loss: 0.9848608374595642, Validation Loss: 1.2712046752373378\n",
      "Epoch: 124/250, Training Loss: 0.7991279363632202, Validation Loss: 1.273504878083865\n",
      "Epoch: 125/250, Training Loss: 0.7818989157676697, Validation Loss: 1.220200002193451\n",
      "Epoch: 126/250, Training Loss: 0.9053305983543396, Validation Loss: 1.2371794780095418\n",
      "Epoch: 127/250, Training Loss: 0.8186140060424805, Validation Loss: 1.1942549347877502\n",
      "Epoch: 128/250, Training Loss: 0.7740280628204346, Validation Loss: 1.2408719311157863\n",
      "Epoch: 129/250, Training Loss: 1.0150914192199707, Validation Loss: 1.2782583634058635\n",
      "Epoch: 130/250, Training Loss: 0.7632044553756714, Validation Loss: 1.189016451438268\n",
      "Epoch: 131/250, Training Loss: 0.8386525511741638, Validation Loss: 1.2719026257594426\n",
      "Epoch: 132/250, Training Loss: 0.8452251553535461, Validation Loss: 1.2665400157372158\n",
      "Epoch: 133/250, Training Loss: 0.8118517398834229, Validation Loss: 1.2460042734940846\n",
      "Epoch: 134/250, Training Loss: 0.9484251737594604, Validation Loss: 1.2367913623650868\n",
      "Epoch: 135/250, Training Loss: 0.8983005285263062, Validation Loss: 1.2947786251703899\n",
      "Epoch: 136/250, Training Loss: 0.8366050124168396, Validation Loss: 1.3181187907854717\n",
      "Epoch: 137/250, Training Loss: 0.9050182104110718, Validation Loss: 1.280466357866923\n",
      "Epoch: 138/250, Training Loss: 0.912651777267456, Validation Loss: 1.2653771539529164\n",
      "Epoch: 139/250, Training Loss: 0.8325397372245789, Validation Loss: 1.215856984257698\n",
      "Epoch: 140/250, Training Loss: 0.8763412833213806, Validation Loss: 1.2417909254630406\n",
      "Epoch: 141/250, Training Loss: 0.7959206700325012, Validation Loss: 1.2222303996483486\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "path = \"../weee/dataset\"\n",
    "batch_size = 256\n",
    "\n",
    "# Create the model\n",
    "model = EnergyExpenditureTPN().to(device)\n",
    "\n",
    "# Copy the weights from the saved MultiTaskTPN model to the new EnergyExpenditureTPN model\n",
    "model.conv1.load_state_dict(loaded_model.conv1.state_dict())\n",
    "model.conv2.load_state_dict(loaded_model.conv2.state_dict())\n",
    "model.conv3.load_state_dict(loaded_model.conv3.state_dict())\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.L1Loss()\n",
    "\n",
    "# Define the optimizer (only optimize the task head parameters)\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "    \n",
    "df = pd.read_csv(f'{path}/combined_e4_acc.csv')\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 250\n",
    "patience = 50\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Iterate over training users\n",
    "    for i in range (14):\n",
    "\n",
    "        # Ignore two subjects because of their missing data\n",
    "        if i+1 in [6, 14]:\n",
    "            continue\n",
    "\n",
    "        train_dataloader = create_dataloader(i, df, batch_size=batch_size)\n",
    "\n",
    "        # Training loop\n",
    "        for batch_idx, (data, labels) in enumerate(train_dataloader):\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(data)\n",
    "\n",
    "            loss = loss_fn(output, labels.view(-1, 1))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # if batch_idx % 100 == 0:\n",
    "            #     print(f\"Epoch: {epoch+1}/{num_epochs}, Batch: {batch_idx}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Iterate over validation users    \n",
    "    val_loss = 0\n",
    "    val_batches = 0\n",
    "    for i in range (14, 16):\n",
    "\n",
    "        validation_dataloader = create_dataloader(i, df, batch_size=batch_size)\n",
    "\n",
    "        # Validation loop\n",
    "        for batch_idx, (data, labels) in enumerate(validation_dataloader):\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "\n",
    "            loss = loss_fn(output, labels.view(-1, 1))\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_batches += 1\n",
    "\n",
    "    val_loss /= val_batches\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}/{num_epochs}, Training Loss: {loss.item()}, Validation Loss: {val_loss}\")\n",
    "\n",
    "    # Save the model if validation loss improves\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"energy_expenditure_tpn_best.pth\")\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    # Early stopping\n",
    "    if epochs_without_improvement == patience:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "    \n",
    "\n",
    "del df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.5277511392320906\n"
     ]
    }
   ],
   "source": [
    "best_model = EnergyExpenditureTPN().to(device)\n",
    "best_model.load_state_dict(torch.load(\"energy_expenditure_tpn_best.pth\"))\n",
    "df = pd.read_csv(f'{path}/combined_e4_acc.csv')\n",
    "\n",
    "testing_dataloader = create_dataloader(16, df, batch_size=batch_size)\n",
    "\n",
    "test_loss = 0\n",
    "for data, labels in testing_dataloader:\n",
    "    data, labels = data.to(device), labels.to(device)\n",
    "    output = best_model(data)\n",
    "    loss = loss_fn(output, labels.view(-1, 1))\n",
    "    test_loss += loss.item()\n",
    "\n",
    "test_loss /= len(testing_dataloader)\n",
    "print(f\"Test Loss: {test_loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
