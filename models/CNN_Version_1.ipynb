{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":652,"status":"ok","timestamp":1681452714221,"user":{"displayName":"Darshana Sandaruwan","userId":"08485959791759341660"},"user_tz":-330},"id":"DDHVRR0L8T6s"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import pandas as pd\n","\n","from torch.utils.data import TensorDataset, DataLoader\n","from sklearn.metrics import accuracy_score, f1_score, r2_score"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cpu\n"]},{"name":"stderr","output_type":"stream","text":["/home/darshana/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:107: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n","  return torch._C._cuda_getDeviceCount() > 0\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def create_windows_np(data, window_size, stride):\n","    num_samples, num_channels = data.shape\n","    num_windows = (num_samples - window_size) // stride + 1\n","\n","    shape = (num_windows, window_size, num_channels)\n","    strides = (data.strides[0] * stride, data.strides[0], data.strides[1])\n","\n","    windows = np.lib.stride_tricks.as_strided(data, shape=shape, strides=strides)\n","\n","    # transpose the windows array to the desired shape\n","    windows = np.transpose(windows, axes=(0, 2, 1))\n","\n","    return windows"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def noise_transform_vectorized(X, sigma=0.05):\n","    \"\"\"\n","    Adding random Gaussian noise with mean 0\n","    \"\"\"\n","    noise = np.random.normal(loc=0, scale=sigma, size=X.shape)\n","    return X + noise\n","\n","def scaling_transform_vectorized(X, sigma=0.1):\n","    \"\"\"\n","    Scaling by a random factor\n","    \"\"\"\n","    scaling_factor = np.random.normal(loc=1.0, scale=sigma, size=(X.shape[0], 1, X.shape[2]))\n","    return X * scaling_factor\n","\n","def negate_transform_vectorized(X):\n","    \"\"\"\n","    Inverting the signals\n","    \"\"\"\n","    return X * -1\n","\n","def time_flip_transform_vectorized(X):\n","    \"\"\"\n","    Reversing the direction of time\n","    \"\"\"\n","    return X[:, ::-1, :]"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# Define the list of transformations to be applied\n","transformations = [\n","    lambda x: noise_transform_vectorized(x), \n","    lambda x: scaling_transform_vectorized(x),\n","    lambda x: negate_transform_vectorized(x),\n","    lambda x: time_flip_transform_vectorized(x),\n","]"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def add_transformations(df):\n","\n","    user_data = create_windows_np(df.loc[:, ['x', 'y', 'z']].values.astype(np.float32), 100, 50)\n","\n","    # Get the number of windows and window size for the user's data\n","    num_windows, _, _ = user_data.shape\n","\n","    # Apply the transformations to the user's data\n","    transformed_data = np.concatenate([transform_fn(user_data) for transform_fn in transformations], axis=0)\n","    transformed_data = np.concatenate([transformed_data, user_data], axis=0)\n","    transformed_data = np.array(transformed_data)\n","\n","    # Create the labels for the transformed data\n","    transformed_labels = np.array([False for _ in range(4)])\n","    transformed_labels = np.append(transformed_labels, True)\n","    transformed_labels = np.repeat(transformed_labels, num_windows)\n","\n","    return {\n","        'X': transformed_data,\n","        'y': transformed_labels\n","    }"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["class MultiTaskTPN(nn.Module):\n","    def __init__(self, num_tasks=len(transformations), num_channels=3):\n","        super(MultiTaskTPN, self).__init__()\n","        self.conv1 = nn.Conv1d(num_channels, 32, kernel_size=24, stride=1)\n","        self.conv2 = nn.Conv1d(32, 64, kernel_size=16, stride=1)\n","        self.conv3 = nn.Conv1d(64, 96, kernel_size=8, stride=1)\n","        self.dropout = nn.Dropout(p=0.1)\n","\n","        self.task_heads = nn.ModuleList([nn.Sequential(\n","            nn.Linear(96, 256),\n","            nn.ReLU(),\n","            nn.Dropout(p=0.1),\n","            nn.Linear(256, 1),\n","            nn.Sigmoid()\n","        ) for _ in range(num_tasks)])\n","\n","    def forward(self, x):\n","        x = self.dropout(nn.functional.relu(self.conv1(x)))\n","        x = self.dropout(nn.functional.relu(self.conv2(x)))\n","        x = self.dropout(nn.functional.relu(self.conv3(x)))\n","        x = nn.functional.max_pool1d(x, x.size(2)).squeeze(2)\n","\n","        logits = [task_head(x).view(-1, 1) for task_head in self.task_heads]\n","        return logits\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1/1, Subject: P001, Batch: 0, Loss: 2.790815830230713\n","Epoch: 1/1, Subject: P001, Batch: 100, Loss: 1.666286826133728\n","Epoch: 1/1, Subject: P001, Batch: 200, Loss: 1.863908052444458\n","Epoch: 1/1, Subject: P001, Batch: 300, Loss: 1.7330677509307861\n","Epoch: 1/1, Subject: P001, Batch: 400, Loss: 1.7743134498596191\n","Epoch: 1/1, Subject: P001, Batch: 500, Loss: 1.8124644756317139\n","Epoch: 1/1, Subject: P001, Batch: 600, Loss: 1.717450499534607\n","Epoch: 1/1, Subject: P001, Batch: 700, Loss: 1.7534329891204834\n","Epoch: 1/1, Subject: P001, Batch: 800, Loss: 1.6110481023788452\n","Epoch: 1/1, Subject: P001, Batch: 900, Loss: 1.7358132600784302\n","Epoch: 1/1, Subject: P001, Batch: 1000, Loss: 1.5451791286468506\n","Epoch: 1/1, Subject: P001, Batch: 1100, Loss: 1.6510359048843384\n","Epoch: 1/1, Subject: P001, Batch: 1200, Loss: 1.6804206371307373\n","Epoch: 1/1, Subject: P001, Batch: 1300, Loss: 1.6861107349395752\n","Epoch: 1/1, Subject: P001, Batch: 1400, Loss: 1.7083008289337158\n","Epoch: 1/1, Subject: P001, Batch: 1500, Loss: 1.8221399784088135\n","Epoch: 1/1, Subject: P001, Batch: 1600, Loss: 1.6599448919296265\n","Epoch: 1/1, Subject: P001, Batch: 1700, Loss: 1.3934624195098877\n","Epoch: 1/1, Subject: P001, Batch: 1800, Loss: 1.651698112487793\n","Epoch: 1/1, Subject: P001, Batch: 1900, Loss: 1.6581472158432007\n","Epoch: 1/1, Subject: P002, Batch: 0, Loss: 1.5956318378448486\n","Epoch: 1/1, Subject: P002, Batch: 100, Loss: 1.6344480514526367\n","Epoch: 1/1, Subject: P002, Batch: 200, Loss: 1.3895622491836548\n","Epoch: 1/1, Subject: P002, Batch: 300, Loss: 1.4234133958816528\n","Epoch: 1/1, Subject: P002, Batch: 400, Loss: 1.3542940616607666\n","Epoch: 1/1, Subject: P002, Batch: 500, Loss: 1.4121372699737549\n","Epoch: 1/1, Subject: P002, Batch: 600, Loss: 1.6146711111068726\n","Epoch: 1/1, Subject: P002, Batch: 700, Loss: 1.524320125579834\n","Epoch: 1/1, Subject: P002, Batch: 800, Loss: 1.4467312097549438\n","Epoch: 1/1, Subject: P002, Batch: 900, Loss: 1.497075080871582\n","Epoch: 1/1, Subject: P002, Batch: 1000, Loss: 1.4991223812103271\n","Epoch: 1/1, Subject: P002, Batch: 1100, Loss: 1.3075156211853027\n","Epoch: 1/1, Subject: P002, Batch: 1200, Loss: 1.3240888118743896\n","Epoch: 1/1, Subject: P002, Batch: 1300, Loss: 1.651678204536438\n","Epoch: 1/1, Subject: P002, Batch: 1400, Loss: 1.6162782907485962\n","Epoch: 1/1, Subject: P002, Batch: 1500, Loss: 1.6346242427825928\n","Epoch: 1/1, Subject: P002, Batch: 1600, Loss: 1.641257405281067\n","Epoch: 1/1, Subject: P002, Batch: 1700, Loss: 1.5755515098571777\n","Epoch: 1/1, Subject: P003, Batch: 0, Loss: 2.2213010787963867\n","Epoch: 1/1, Subject: P003, Batch: 100, Loss: 1.5296471118927002\n","Epoch: 1/1, Subject: P003, Batch: 200, Loss: 1.533064603805542\n","Epoch: 1/1, Subject: P003, Batch: 300, Loss: 1.6337064504623413\n","Epoch: 1/1, Subject: P003, Batch: 400, Loss: 1.2606861591339111\n","Epoch: 1/1, Subject: P003, Batch: 500, Loss: 1.4919425249099731\n","Epoch: 1/1, Subject: P003, Batch: 600, Loss: 1.4755358695983887\n","Epoch: 1/1, Subject: P003, Batch: 700, Loss: 1.4473986625671387\n","Epoch: 1/1, Subject: P003, Batch: 800, Loss: 1.4201871156692505\n","Epoch: 1/1, Subject: P003, Batch: 900, Loss: 1.4810280799865723\n","Epoch: 1/1, Subject: P003, Batch: 1000, Loss: 1.5075349807739258\n","Epoch: 1/1, Subject: P003, Batch: 1100, Loss: 1.3104362487792969\n","Epoch: 1/1, Subject: P003, Batch: 1200, Loss: 1.2354905605316162\n","Epoch: 1/1, Subject: P003, Batch: 1300, Loss: 1.0771434307098389\n","Epoch: 1/1, Subject: P003, Batch: 1400, Loss: 1.0707173347473145\n","Epoch: 1/1, Subject: P003, Batch: 1500, Loss: 1.067983865737915\n","Epoch: 1/1, Subject: P003, Batch: 1600, Loss: 1.0438344478607178\n","Epoch: 1/1, Subject: P003, Batch: 1700, Loss: 0.9674661159515381\n","Epoch: 1/1, Subject: P003, Batch: 1800, Loss: 1.2013134956359863\n","Epoch: 1/1, Subject: P004, Batch: 0, Loss: 1.3702504634857178\n","Epoch: 1/1, Subject: P004, Batch: 100, Loss: 1.151532530784607\n","Epoch: 1/1, Subject: P004, Batch: 200, Loss: 0.9904888868331909\n","Epoch: 1/1, Subject: P004, Batch: 300, Loss: 1.0538098812103271\n","Epoch: 1/1, Subject: P004, Batch: 400, Loss: 1.3468531370162964\n","Epoch: 1/1, Subject: P004, Batch: 500, Loss: 0.9576787948608398\n","Epoch: 1/1, Subject: P004, Batch: 600, Loss: 0.9416456818580627\n","Epoch: 1/1, Subject: P004, Batch: 700, Loss: 1.2749347686767578\n","Epoch: 1/1, Subject: P004, Batch: 800, Loss: 0.8715547323226929\n","Epoch: 1/1, Subject: P004, Batch: 900, Loss: 1.2062394618988037\n","Epoch: 1/1, Subject: P004, Batch: 1000, Loss: 0.6420292258262634\n","Epoch: 1/1, Subject: P004, Batch: 1100, Loss: 0.7768431901931763\n","Epoch: 1/1, Subject: P004, Batch: 1200, Loss: 0.9139024615287781\n","Epoch: 1/1, Subject: P004, Batch: 1300, Loss: 0.9989084005355835\n","Epoch: 1/1, Subject: P004, Batch: 1400, Loss: 0.7674190998077393\n","Epoch: 1/1, Subject: P004, Batch: 1500, Loss: 0.9532146453857422\n","Epoch: 1/1, Subject: P005, Batch: 0, Loss: 1.1146924495697021\n","Epoch: 1/1, Subject: P005, Batch: 100, Loss: 0.9847946763038635\n","Epoch: 1/1, Subject: P005, Batch: 200, Loss: 0.8704771995544434\n","Epoch: 1/1, Subject: P005, Batch: 300, Loss: 0.8469362854957581\n","Epoch: 1/1, Subject: P005, Batch: 400, Loss: 0.6261895298957825\n","Epoch: 1/1, Subject: P005, Batch: 500, Loss: 0.9198193550109863\n","Epoch: 1/1, Subject: P005, Batch: 600, Loss: 0.8107767105102539\n","Epoch: 1/1, Subject: P005, Batch: 700, Loss: 0.8930205702781677\n","Epoch: 1/1, Subject: P005, Batch: 800, Loss: 0.705656111240387\n","Epoch: 1/1, Subject: P005, Batch: 900, Loss: 0.8107961416244507\n","Epoch: 1/1, Subject: P005, Batch: 1000, Loss: 0.9213171601295471\n","Epoch: 1/1, Subject: P005, Batch: 1100, Loss: 0.9967722296714783\n","Epoch: 1/1, Subject: P005, Batch: 1200, Loss: 0.9282198548316956\n","Epoch: 1/1, Subject: P005, Batch: 1300, Loss: 0.9033001065254211\n","Epoch: 1/1, Subject: P005, Batch: 1400, Loss: 0.7430736422538757\n","Epoch: 1/1, Subject: P005, Batch: 1500, Loss: 0.9929087162017822\n","Epoch: 1/1, Subject: P005, Batch: 1600, Loss: 0.8570947051048279\n","Epoch: 1/1, Subject: P005, Batch: 1700, Loss: 0.9353393316268921\n","Epoch: 1/1, Subject: P005, Batch: 1800, Loss: 0.9256740212440491\n","Epoch: 1/1, Subject: P005, Batch: 1900, Loss: 0.8683207035064697\n","Epoch: 1/1, Subject: P006, Batch: 0, Loss: 0.9351068735122681\n","Epoch: 1/1, Subject: P006, Batch: 100, Loss: 0.8548731803894043\n","Epoch: 1/1, Subject: P006, Batch: 200, Loss: 0.7205367088317871\n","Epoch: 1/1, Subject: P006, Batch: 300, Loss: 0.8401309251785278\n","Epoch: 1/1, Subject: P006, Batch: 400, Loss: 0.6798378825187683\n","Epoch: 1/1, Subject: P006, Batch: 500, Loss: 0.9967549443244934\n","Epoch: 1/1, Subject: P006, Batch: 600, Loss: 0.8571327328681946\n","Epoch: 1/1, Subject: P006, Batch: 700, Loss: 0.6177533864974976\n","Epoch: 1/1, Subject: P006, Batch: 800, Loss: 0.5666837096214294\n","Epoch: 1/1, Subject: P006, Batch: 900, Loss: 0.9373128414154053\n","Epoch: 1/1, Subject: P006, Batch: 1000, Loss: 0.7549952268600464\n","Epoch: 1/1, Subject: P006, Batch: 1100, Loss: 0.6531877517700195\n","Epoch: 1/1, Subject: P006, Batch: 1200, Loss: 0.7041668891906738\n","Epoch: 1/1, Subject: P006, Batch: 1300, Loss: 0.6221552491188049\n","Epoch: 1/1, Subject: P006, Batch: 1400, Loss: 0.6663190126419067\n","Epoch: 1/1, Subject: P006, Batch: 1500, Loss: 0.8105670809745789\n","Epoch: 1/1, Subject: P006, Batch: 1600, Loss: 0.6727167963981628\n","Epoch: 1/1, Subject: P006, Batch: 1700, Loss: 0.6901131272315979\n","Epoch: 1/1, Subject: P006, Batch: 1800, Loss: 0.5062015056610107\n","Epoch: 1/1, Subject: P006, Batch: 1900, Loss: 0.6733718514442444\n","Epoch: 1/1, Subject: P006, Batch: 2000, Loss: 0.7399677038192749\n","Epoch: 1/1, Subject: P006, Batch: 2100, Loss: 0.8288986682891846\n","Epoch: 1/1, Subject: P006, Batch: 2200, Loss: 0.5447602272033691\n","Epoch: 1/1, Subject: P007, Batch: 0, Loss: 1.0795342922210693\n","Epoch: 1/1, Subject: P007, Batch: 100, Loss: 1.0123047828674316\n","Epoch: 1/1, Subject: P007, Batch: 200, Loss: 0.8110240697860718\n","Epoch: 1/1, Subject: P007, Batch: 300, Loss: 0.8389473557472229\n","Epoch: 1/1, Subject: P007, Batch: 400, Loss: 0.8844997882843018\n","Epoch: 1/1, Subject: P007, Batch: 500, Loss: 0.8206071257591248\n","Epoch: 1/1, Subject: P007, Batch: 600, Loss: 0.8935849070549011\n","Epoch: 1/1, Subject: P007, Batch: 700, Loss: 0.8696604371070862\n","Epoch: 1/1, Subject: P007, Batch: 800, Loss: 0.7157825827598572\n","Epoch: 1/1, Subject: P007, Batch: 900, Loss: 0.9832055568695068\n","Epoch: 1/1, Subject: P007, Batch: 1000, Loss: 0.7170001864433289\n","Epoch: 1/1, Subject: P007, Batch: 1100, Loss: 0.9863905906677246\n","Epoch: 1/1, Subject: P007, Batch: 1200, Loss: 0.8042430877685547\n","Epoch: 1/1, Subject: P007, Batch: 1300, Loss: 0.7029587030410767\n","Epoch: 1/1, Subject: P007, Batch: 1400, Loss: 1.010617733001709\n","Epoch: 1/1, Subject: P007, Batch: 1500, Loss: 0.8447287082672119\n","Epoch: 1/1, Subject: P007, Batch: 1600, Loss: 0.8764070272445679\n","Epoch: 1/1, Subject: P007, Batch: 1700, Loss: 0.8005006313323975\n","Epoch: 1/1, Subject: P007, Batch: 1800, Loss: 0.7031241655349731\n","Epoch: 1/1, Subject: P007, Batch: 1900, Loss: 0.9233477115631104\n","Epoch: 1/1, Subject: P009, Batch: 0, Loss: 1.0373821258544922\n","Epoch: 1/1, Subject: P009, Batch: 100, Loss: 0.8669806122779846\n","Epoch: 1/1, Subject: P009, Batch: 200, Loss: 0.8531514406204224\n","Epoch: 1/1, Subject: P009, Batch: 300, Loss: 0.9723738431930542\n","Epoch: 1/1, Subject: P009, Batch: 400, Loss: 0.7876452803611755\n","Epoch: 1/1, Subject: P009, Batch: 500, Loss: 0.7527984976768494\n","Epoch: 1/1, Subject: P009, Batch: 600, Loss: 0.7763573527336121\n","Epoch: 1/1, Subject: P009, Batch: 700, Loss: 0.8866344690322876\n","Epoch: 1/1, Subject: P009, Batch: 800, Loss: 0.8303595781326294\n","Epoch: 1/1, Subject: P009, Batch: 900, Loss: 0.9363206624984741\n","Epoch: 1/1, Subject: P009, Batch: 1000, Loss: 0.7934450507164001\n","Epoch: 1/1, Subject: P009, Batch: 1100, Loss: 0.6680954694747925\n","Epoch: 1/1, Subject: P009, Batch: 1200, Loss: 0.6474837064743042\n","Epoch: 1/1, Subject: P009, Batch: 1300, Loss: 0.9364694952964783\n","Epoch: 1/1, Subject: P009, Batch: 1400, Loss: 0.5483869314193726\n","Epoch: 1/1, Subject: P010, Batch: 0, Loss: 1.1764039993286133\n","Epoch: 1/1, Subject: P010, Batch: 100, Loss: 0.8652193546295166\n","Epoch: 1/1, Subject: P010, Batch: 200, Loss: 0.9514812231063843\n","Epoch: 1/1, Subject: P010, Batch: 300, Loss: 0.8775825500488281\n","Epoch: 1/1, Subject: P010, Batch: 400, Loss: 0.8389096260070801\n","Epoch: 1/1, Subject: P010, Batch: 500, Loss: 0.7877979278564453\n","Epoch: 1/1, Subject: P010, Batch: 600, Loss: 0.7049471139907837\n","Epoch: 1/1, Subject: P010, Batch: 700, Loss: 0.8319204449653625\n","Epoch: 1/1, Subject: P010, Batch: 800, Loss: 0.6638223528862\n","Epoch: 1/1, Subject: P010, Batch: 900, Loss: 0.8016852140426636\n","Epoch: 1/1, Subject: P010, Batch: 1000, Loss: 0.815414547920227\n","Epoch: 1/1, Subject: P010, Batch: 1100, Loss: 0.9838800430297852\n","Epoch: 1/1, Subject: P010, Batch: 1200, Loss: 0.775558352470398\n","Epoch: 1/1, Subject: P010, Batch: 1300, Loss: 0.6615486741065979\n","Epoch: 1/1, Subject: P010, Batch: 1400, Loss: 0.6835483312606812\n","Epoch: 1/1, Subject: P010, Batch: 1500, Loss: 0.7725425958633423\n","Epoch: 1/1, Subject: P010, Batch: 1600, Loss: 0.7572774887084961\n","Epoch: 1/1, Subject: P010, Batch: 1700, Loss: 0.7502651214599609\n","Epoch: 1/1, Subject: P011, Batch: 0, Loss: 0.7205514907836914\n","Epoch: 1/1, Subject: P011, Batch: 100, Loss: 0.5920947790145874\n","Epoch: 1/1, Subject: P011, Batch: 200, Loss: 0.5606119632720947\n","Epoch: 1/1, Subject: P011, Batch: 300, Loss: 0.6302855014801025\n","Epoch: 1/1, Subject: P011, Batch: 400, Loss: 0.8232674598693848\n","Epoch: 1/1, Subject: P011, Batch: 500, Loss: 0.6808884143829346\n","Epoch: 1/1, Subject: P011, Batch: 600, Loss: 1.010941743850708\n","Epoch: 1/1, Subject: P011, Batch: 700, Loss: 0.5355343818664551\n","Epoch: 1/1, Subject: P011, Batch: 800, Loss: 0.46602535247802734\n","Epoch: 1/1, Subject: P011, Batch: 900, Loss: 0.7648093700408936\n","Epoch: 1/1, Subject: P011, Batch: 1000, Loss: 0.5208858251571655\n","Epoch: 1/1, Subject: P011, Batch: 1100, Loss: 0.5692201852798462\n","Epoch: 1/1, Subject: P011, Batch: 1200, Loss: 1.4015135765075684\n","Epoch: 1/1, Subject: P011, Batch: 1300, Loss: 0.6007925271987915\n","Epoch: 1/1, Subject: P011, Batch: 1400, Loss: 0.42169877886772156\n","Epoch: 1/1, Subject: P011, Batch: 1500, Loss: 0.5826786756515503\n","Epoch: 1/1, Subject: P011, Batch: 1600, Loss: 0.6247475147247314\n","Epoch: 1/1, Subject: P011, Batch: 1700, Loss: 0.48038703203201294\n","Epoch: 1/1, Subject: P011, Batch: 1800, Loss: 0.6217908263206482\n","Epoch: 1/1, Subject: P012, Batch: 0, Loss: 0.886605441570282\n","Epoch: 1/1, Subject: P012, Batch: 100, Loss: 0.6399285197257996\n","Epoch: 1/1, Subject: P012, Batch: 200, Loss: 0.5809398889541626\n","Epoch: 1/1, Subject: P012, Batch: 300, Loss: 0.7669793367385864\n","Epoch: 1/1, Subject: P012, Batch: 400, Loss: 0.5800179839134216\n","Epoch: 1/1, Subject: P012, Batch: 500, Loss: 0.7993018627166748\n","Epoch: 1/1, Subject: P012, Batch: 600, Loss: 0.5352596640586853\n","Epoch: 1/1, Subject: P012, Batch: 700, Loss: 0.5406737327575684\n","Epoch: 1/1, Subject: P012, Batch: 800, Loss: 0.49446171522140503\n","Epoch: 1/1, Subject: P012, Batch: 900, Loss: 0.677841067314148\n","Epoch: 1/1, Subject: P012, Batch: 1000, Loss: 0.506024956703186\n","Epoch: 1/1, Subject: P012, Batch: 1100, Loss: 0.6795194745063782\n","Epoch: 1/1, Subject: P012, Batch: 1200, Loss: 0.683782160282135\n","Epoch: 1/1, Subject: P012, Batch: 1300, Loss: 0.6653367877006531\n","Epoch: 1/1, Subject: P012, Batch: 1400, Loss: 1.0695176124572754\n","Epoch: 1/1, Subject: P012, Batch: 1500, Loss: 0.7394176721572876\n","Epoch: 1/1, Subject: P012, Batch: 1600, Loss: 0.9073631763458252\n","Epoch: 1/1, Subject: P012, Batch: 1700, Loss: 0.7939084768295288\n","Epoch: 1/1, Subject: P012, Batch: 1800, Loss: 0.7134445905685425\n","Epoch: 1/1, Subject: P013, Batch: 0, Loss: 1.195919156074524\n","Epoch: 1/1, Subject: P013, Batch: 100, Loss: 0.5847909450531006\n","Epoch: 1/1, Subject: P013, Batch: 200, Loss: 0.9357293248176575\n","Epoch: 1/1, Subject: P013, Batch: 300, Loss: 0.7882727384567261\n","Epoch: 1/1, Subject: P013, Batch: 400, Loss: 0.6664500832557678\n","Epoch: 1/1, Subject: P013, Batch: 500, Loss: 0.6708096861839294\n","Epoch: 1/1, Subject: P013, Batch: 600, Loss: 0.6303088068962097\n","Epoch: 1/1, Subject: P013, Batch: 700, Loss: 0.5452307462692261\n","Epoch: 1/1, Subject: P013, Batch: 800, Loss: 0.6856493949890137\n","Epoch: 1/1, Subject: P013, Batch: 900, Loss: 0.7953171133995056\n","Epoch: 1/1, Subject: P013, Batch: 1000, Loss: 0.692468523979187\n","Epoch: 1/1, Subject: P013, Batch: 1100, Loss: 0.701607346534729\n","Epoch: 1/1, Subject: P013, Batch: 1200, Loss: 0.5127700567245483\n","Epoch: 1/1, Subject: P013, Batch: 1300, Loss: 0.6915189027786255\n","Epoch: 1/1, Subject: P013, Batch: 1400, Loss: 0.47453534603118896\n","Epoch: 1/1, Subject: P013, Batch: 1500, Loss: 0.6864884495735168\n","Epoch: 1/1, Subject: P013, Batch: 1600, Loss: 0.5351741313934326\n","Epoch: 1/1, Subject: P014, Batch: 0, Loss: 0.9670294523239136\n","Epoch: 1/1, Subject: P014, Batch: 100, Loss: 0.6910232901573181\n","Epoch: 1/1, Subject: P014, Batch: 200, Loss: 0.5531453490257263\n","Epoch: 1/1, Subject: P014, Batch: 300, Loss: 0.547687292098999\n","Epoch: 1/1, Subject: P014, Batch: 400, Loss: 0.5961840748786926\n","Epoch: 1/1, Subject: P014, Batch: 500, Loss: 0.5272709131240845\n","Epoch: 1/1, Subject: P014, Batch: 600, Loss: 0.5740773677825928\n","Epoch: 1/1, Subject: P014, Batch: 700, Loss: 0.6307365894317627\n","Epoch: 1/1, Subject: P014, Batch: 800, Loss: 0.8046331405639648\n","Epoch: 1/1, Subject: P014, Batch: 900, Loss: 0.6898297071456909\n","Epoch: 1/1, Subject: P014, Batch: 1000, Loss: 0.505574643611908\n","Epoch: 1/1, Subject: P014, Batch: 1100, Loss: 0.4640764594078064\n","Epoch: 1/1, Subject: P014, Batch: 1200, Loss: 0.6824738383293152\n","Epoch: 1/1, Subject: P014, Batch: 1300, Loss: 0.5242855548858643\n","Epoch: 1/1, Subject: P014, Batch: 1400, Loss: 0.8780921697616577\n","Epoch: 1/1, Subject: P014, Batch: 1500, Loss: 0.540251612663269\n","Epoch: 1/1, Subject: P014, Batch: 1600, Loss: 0.6889899969100952\n","Epoch: 1/1, Subject: P014, Batch: 1700, Loss: 0.5374559164047241\n","Epoch: 1/1, Subject: P014, Batch: 1800, Loss: 0.6162748336791992\n","Epoch: 1/1, Subject: P014, Batch: 1900, Loss: 0.47263699769973755\n","Epoch: 1/1, Subject: P015, Batch: 0, Loss: 1.2004873752593994\n","Epoch: 1/1, Subject: P015, Batch: 100, Loss: 0.8321018218994141\n","Epoch: 1/1, Subject: P015, Batch: 200, Loss: 0.5423903465270996\n","Epoch: 1/1, Subject: P015, Batch: 300, Loss: 0.628261923789978\n","Epoch: 1/1, Subject: P015, Batch: 400, Loss: 0.6180306673049927\n","Epoch: 1/1, Subject: P015, Batch: 500, Loss: 0.7109681963920593\n","Epoch: 1/1, Subject: P015, Batch: 600, Loss: 0.6510654091835022\n","Epoch: 1/1, Subject: P015, Batch: 700, Loss: 0.7817201614379883\n","Epoch: 1/1, Subject: P015, Batch: 800, Loss: 0.6395933628082275\n","Epoch: 1/1, Subject: P015, Batch: 900, Loss: 0.5726807713508606\n","Epoch: 1/1, Subject: P015, Batch: 1000, Loss: 0.8640670776367188\n","Epoch: 1/1, Subject: P015, Batch: 1100, Loss: 0.45854252576828003\n","Epoch: 1/1, Subject: P015, Batch: 1200, Loss: 0.4473109841346741\n","Epoch: 1/1, Subject: P015, Batch: 1300, Loss: 0.6699725389480591\n","Epoch: 1/1, Subject: P015, Batch: 1400, Loss: 0.6655941605567932\n","Epoch: 1/1, Subject: P015, Batch: 1500, Loss: 0.7524829506874084\n","Epoch: 1/1, Subject: P015, Batch: 1600, Loss: 0.8575434684753418\n","Epoch: 1/1, Subject: P015, Batch: 1700, Loss: 0.5774508714675903\n","Epoch: 1/1, Subject: P015, Batch: 1800, Loss: 0.4210440218448639\n","Epoch: 1/1, Subject: P018, Batch: 0, Loss: 1.417889952659607\n","Epoch: 1/1, Subject: P018, Batch: 100, Loss: 0.8500121831893921\n","Epoch: 1/1, Subject: P018, Batch: 200, Loss: 0.6948976516723633\n","Epoch: 1/1, Subject: P018, Batch: 300, Loss: 0.8533828258514404\n","Epoch: 1/1, Subject: P018, Batch: 400, Loss: 0.6970889568328857\n","Epoch: 1/1, Subject: P018, Batch: 500, Loss: 0.586555540561676\n","Epoch: 1/1, Subject: P018, Batch: 600, Loss: 0.5178037285804749\n","Epoch: 1/1, Subject: P018, Batch: 700, Loss: 0.46791496872901917\n","Epoch: 1/1, Subject: P018, Batch: 800, Loss: 0.7125065922737122\n","Epoch: 1/1, Subject: P018, Batch: 900, Loss: 0.7841290235519409\n","Epoch: 1/1, Subject: P018, Batch: 1000, Loss: 0.5911029577255249\n","Epoch: 1/1, Subject: P018, Batch: 1100, Loss: 0.7374016046524048\n","Epoch: 1/1, Subject: P018, Batch: 1200, Loss: 0.6840766668319702\n","Epoch: 1/1, Subject: P018, Batch: 1300, Loss: 0.6039373874664307\n","Epoch: 1/1, Subject: P018, Batch: 1400, Loss: 0.6210362315177917\n","Epoch: 1/1, Subject: P018, Batch: 1500, Loss: 0.7710855007171631\n","Epoch: 1/1, Subject: P018, Batch: 1600, Loss: 0.6343781352043152\n","Epoch: 1/1, Subject: P018, Batch: 1700, Loss: 0.5356401205062866\n","Epoch: 1/1, Subject: P019, Batch: 0, Loss: 1.69101881980896\n","Epoch: 1/1, Subject: P019, Batch: 100, Loss: 0.8794773817062378\n","Epoch: 1/1, Subject: P019, Batch: 200, Loss: 0.5936911702156067\n","Epoch: 1/1, Subject: P019, Batch: 300, Loss: 0.7105645537376404\n","Epoch: 1/1, Subject: P019, Batch: 400, Loss: 0.5919345021247864\n","Epoch: 1/1, Subject: P019, Batch: 500, Loss: 0.8081732392311096\n","Epoch: 1/1, Subject: P019, Batch: 600, Loss: 0.6051496863365173\n","Epoch: 1/1, Subject: P019, Batch: 700, Loss: 0.7573189735412598\n","Epoch: 1/1, Subject: P019, Batch: 800, Loss: 0.5077724456787109\n","Epoch: 1/1, Subject: P019, Batch: 900, Loss: 0.5308066606521606\n","Epoch: 1/1, Subject: P019, Batch: 1000, Loss: 0.688269853591919\n","Epoch: 1/1, Subject: P019, Batch: 1100, Loss: 0.6253887414932251\n","Epoch: 1/1, Subject: P019, Batch: 1200, Loss: 0.5878797769546509\n","Epoch: 1/1, Subject: P019, Batch: 1300, Loss: 0.654010534286499\n","Epoch: 1/1, Subject: P019, Batch: 1400, Loss: 0.6134812831878662\n","Epoch: 1/1, Subject: P019, Batch: 1500, Loss: 0.5374414920806885\n","Epoch: 1/1, Subject: P019, Batch: 1600, Loss: 0.6367363333702087\n","Epoch: 1/1, Subject: P019, Batch: 1700, Loss: 0.796113133430481\n","Epoch: 1/1, Subject: P019, Batch: 1800, Loss: 0.6316781044006348\n","Epoch: 1/1, Subject: P020, Batch: 0, Loss: 0.6794389486312866\n","Epoch: 1/1, Subject: P020, Batch: 100, Loss: 0.4428272247314453\n","Epoch: 1/1, Subject: P020, Batch: 200, Loss: 0.49143698811531067\n","Epoch: 1/1, Subject: P020, Batch: 300, Loss: 0.7710342407226562\n","Epoch: 1/1, Subject: P020, Batch: 400, Loss: 0.5624098777770996\n","Epoch: 1/1, Subject: P020, Batch: 500, Loss: 0.6040669679641724\n","Epoch: 1/1, Subject: P020, Batch: 600, Loss: 0.5446616411209106\n","Epoch: 1/1, Subject: P020, Batch: 700, Loss: 0.4935075044631958\n","Epoch: 1/1, Subject: P020, Batch: 800, Loss: 0.46983590722084045\n","Epoch: 1/1, Subject: P020, Batch: 900, Loss: 0.6159437298774719\n","Epoch: 1/1, Subject: P020, Batch: 1000, Loss: 0.52842116355896\n","Epoch: 1/1, Subject: P020, Batch: 1100, Loss: 0.5115806460380554\n","Epoch: 1/1, Subject: P020, Batch: 1200, Loss: 0.4297313690185547\n","Epoch: 1/1, Subject: P020, Batch: 1300, Loss: 0.5047346353530884\n","Epoch: 1/1, Subject: P020, Batch: 1400, Loss: 0.6187387704849243\n","Epoch: 1/1, Subject: P020, Batch: 1500, Loss: 0.6060431599617004\n"]}],"source":["path = \"../capture24\"\n","batch_size = 256\n","\n","# Create the model\n","model = MultiTaskTPN().to(device)\n","\n","# Define the loss function and optimizer\n","loss_fn = nn.BCELoss()\n","\n","# Define the optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n","\n","# Training loop\n","num_epochs = 1\n","for epoch in range(num_epochs):\n","    \n","    for i in range(20):\n","\n","        # ignore subjects with missing values\n","        if i == 7 or i == 15 or i == 16:\n","            continue\n","\n","        df = pd.read_feather(f'{path}/training.feather', columns=['x', 'y', 'z', 'user_id'])\n","\n","        # Get the data for the current user\n","        user_data = add_transformations(df.loc[df['user_id'] == i+1])\n","\n","        # Create a DataLoader to iterate over the test data in batches\n","        train_dataset = TensorDataset(torch.tensor(user_data['X'], dtype=torch.float32), \n","                                    torch.tensor(user_data['y'], dtype=torch.float32))\n","        \n","        # Create a DataLoader to iterate over the test data in batches\n","        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","        \n","\n","        # Training loop\n","        for batch_idx, (data, labels) in enumerate(train_dataloader):\n","            data, labels = data.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","\n","            logits = model(data)\n","\n","            losses = []\n","\n","            # Calculate the loss for each task\n","            for logit in logits:\n","                losses.append(loss_fn(logit, labels.view(-1, 1)))\n","            \n","            # Sum the losses to get the total loss\n","            total_loss = sum(losses)\n","            \n","            total_loss.backward()\n","            optimizer.step()\n","            \n","            if batch_idx % 100 == 0:\n","                print(f\"Epoch: {epoch+1}/{num_epochs}, Subject: P{i+1:03d}, Batch: {batch_idx}, Loss: {total_loss.item()}\")\n","\n","        del df\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# Save the model's state dictionary\n","torch.save(model.state_dict(), 'multi_task_tpn.pth')"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["def evaluate(model, dataloader, device):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    accuracy = 0\n","\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for data, labels in dataloader:\n","            data, labels = data.to(device), labels.to(device)\n","\n","            logits = model(data)\n","\n","            # Concatenate the logits from each task\n","            combined_logits = np.concatenate([logit.cpu().numpy() for logit in logits], axis=1)\n","\n","            # Calculate the accuracy by comparing the highest logit with the correct label\n","            pred = np.around(np.amax(combined_logits, axis=1))\n","            correct += np.sum(pred == labels.cpu().numpy())\n","            total += len(labels)\n","\n","            all_preds.extend(pred)\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    accuracy = correct / total\n","    f1 = f1_score(all_labels, all_preds, average='weighted')\n","    return accuracy, f1"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Validation accuracy: 90.81%, F1: 90.35%\n"]}],"source":["val_df = pd.read_feather(f'{path}/test.feather', columns=['x', 'y', 'z', 'user_id'])\n","\n","# Get the validation data and apply transformations\n","val_data = add_transformations(val_df)\n","    \n","# Create a DataLoader to iterate over the validation data in batches\n","val_dataset = TensorDataset(torch.tensor(val_data['X'], dtype=torch.float32), \n","                            torch.tensor(val_data['y'], dtype=torch.float32))\n","val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n","\n","val_accuracy, val_f1 = evaluate(model, val_dataloader, device)\n","print(f\"Validation accuracy: {val_accuracy * 100:.2f}%, F1: {val_f1 * 100:.2f}%\")\n","\n","del val_df\n","del val_data\n","del val_dataset\n","del val_dataloader\n","torch.cuda.empty_cache()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":0}
