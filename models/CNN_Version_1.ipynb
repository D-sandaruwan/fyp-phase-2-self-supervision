{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":652,"status":"ok","timestamp":1681452714221,"user":{"displayName":"Darshana Sandaruwan","userId":"08485959791759341660"},"user_tz":-330},"id":"DDHVRR0L8T6s"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import pandas as pd\n","\n","from torch.utils.data import TensorDataset, DataLoader\n","from sklearn.metrics import accuracy_score, f1_score, r2_score\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def create_windows_np(data, window_size, stride):\n","    num_samples, num_channels = data.shape\n","    num_windows = (num_samples - window_size) // stride + 1\n","\n","    shape = (num_windows, window_size, num_channels)\n","    strides = (data.strides[0] * stride, data.strides[0], data.strides[1])\n","\n","    windows = np.lib.stride_tricks.as_strided(data, shape=shape, strides=strides)\n","\n","    # transpose the windows array to the desired shape\n","    windows = np.transpose(windows, axes=(0, 2, 1))\n","\n","    return windows"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def noise_transform_vectorized(X, sigma=0.05):\n","    \"\"\"\n","    Adding random Gaussian noise with mean 0\n","    \"\"\"\n","    noise = np.random.normal(loc=0, scale=sigma, size=X.shape)\n","    return X + noise\n","\n","def scaling_transform_vectorized(X, sigma=0.1):\n","    \"\"\"\n","    Scaling by a random factor\n","    \"\"\"\n","    scaling_factor = np.random.normal(loc=1.0, scale=sigma, size=(X.shape[0], 1, X.shape[2]))\n","    return X * scaling_factor\n","\n","def negate_transform_vectorized(X):\n","    \"\"\"\n","    Inverting the signals\n","    \"\"\"\n","    return X * -1\n","\n","def time_flip_transform_vectorized(X):\n","    \"\"\"\n","    Reversing the direction of time\n","    \"\"\"\n","    return X[:, ::-1, :]"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# Define the list of transformations to be applied\n","transformations = [\n","    lambda x: noise_transform_vectorized(x), \n","    lambda x: scaling_transform_vectorized(x),\n","    lambda x: negate_transform_vectorized(x),\n","    lambda x: time_flip_transform_vectorized(x),\n","]"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def add_transformations(df):\n","\n","    user_data = create_windows_np(df.loc[:, ['x', 'y', 'z']].values.astype(np.float32), 100, 50)\n","\n","    # Get the number of windows and window size for the user's data\n","    num_windows, _, _ = user_data.shape\n","\n","    # Apply the transformations to the user's data\n","    transformed_data = np.concatenate([transform_fn(user_data) for transform_fn in transformations], axis=0)\n","    transformed_data = np.concatenate([transformed_data, user_data], axis=0)\n","    transformed_data = np.array(transformed_data)\n","\n","    # Create the labels for the transformed data\n","    transformed_labels = np.array([False for _ in range(4)])\n","    transformed_labels = np.append(transformed_labels, True)\n","    transformed_labels = np.repeat(transformed_labels, num_windows)\n","\n","    return {\n","        'X': transformed_data,\n","        'y': transformed_labels\n","    }"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["class MultiTaskTPN(nn.Module):\n","    def __init__(self, num_tasks=len(transformations), num_channels=3):\n","        super(MultiTaskTPN, self).__init__()\n","        self.conv1 = nn.Conv1d(num_channels, 32, kernel_size=24, stride=1)\n","        self.conv2 = nn.Conv1d(32, 64, kernel_size=16, stride=1)\n","        self.conv3 = nn.Conv1d(64, 96, kernel_size=8, stride=1)\n","        self.dropout = nn.Dropout(p=0.1)\n","\n","        self.task_heads = nn.ModuleList([nn.Sequential(\n","            nn.Linear(96, 256),\n","            nn.ReLU(),\n","            nn.Dropout(p=0.1),\n","            nn.Linear(256, 1),\n","            nn.Sigmoid()\n","        ) for _ in range(num_tasks)])\n","\n","    def forward(self, x):\n","        x = self.dropout(nn.functional.relu(self.conv1(x)))\n","        x = self.dropout(nn.functional.relu(self.conv2(x)))\n","        x = self.dropout(nn.functional.relu(self.conv3(x)))\n","        x = nn.functional.max_pool1d(x, x.size(2)).squeeze(2)\n","\n","        logits = [task_head(x).view(-1, 1) for task_head in self.task_heads]\n","        return logits\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m9\u001b[39m, \u001b[39m10\u001b[39m):\n\u001b[1;32m     17\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_feather(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m/training.feather\u001b[39m\u001b[39m'\u001b[39m, columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mz\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39muser_id\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 18\u001b[0m     user_data \u001b[39m=\u001b[39m add_transformations(df\u001b[39m.\u001b[39;49mloc[df[\u001b[39m'\u001b[39;49m\u001b[39muser_id\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m==\u001b[39;49m i\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m])\n\u001b[1;32m     19\u001b[0m     \u001b[39mprint\u001b[39m(user_data[\u001b[39m'\u001b[39m\u001b[39mX\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     20\u001b[0m     \u001b[39mprint\u001b[39m(user_data[\u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshape)\n","Cell \u001b[0;32mIn[10], line 9\u001b[0m, in \u001b[0;36madd_transformations\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      6\u001b[0m num_windows, _, _ \u001b[39m=\u001b[39m user_data\u001b[39m.\u001b[39mshape\n\u001b[1;32m      8\u001b[0m \u001b[39m# Apply the transformations to the user's data\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m transformed_data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mconcatenate([transform_fn(user_data) \u001b[39mfor\u001b[39;49;00m transform_fn \u001b[39min\u001b[39;49;00m transformations], axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     10\u001b[0m transformed_data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate([transformed_data, user_data], axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     11\u001b[0m transformed_data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(transformed_data)\n","File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["path = \"../capture24\"\n","batch_size = 256\n","\n","# Create the model\n","model = MultiTaskTPN().to(device)\n","\n","# Define the loss function and optimizer\n","loss_fn = nn.BCELoss()\n","\n","# Define the optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n","\n","# Training loop\n","num_epochs = 1\n","for epoch in range(num_epochs):\n","    \n","    for i in range(9, 10):\n","\n","        # ignore subjects with missing values\n","        if i == 7 or i == 15 or i == 16:\n","            continue\n","\n","        df = pd.read_feather(f'{path}/training.feather', columns=['x', 'y', 'z', 'user_id'])\n","\n","        # Get the data for the current user\n","        user_data = add_transformations(df.loc[df['user_id'] == i+1])\n","\n","        # Create a DataLoader to iterate over the test data in batches\n","        train_dataset = TensorDataset(torch.tensor(user_data['X'], dtype=torch.float32), \n","                                    torch.tensor(user_data['y'], dtype=torch.float32))\n","        \n","        # Create a DataLoader to iterate over the test data in batches\n","        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","        \n","\n","        # Training loop\n","        for batch_idx, (data, labels) in enumerate(train_dataloader):\n","            data, labels = data.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","\n","            logits = model(data)\n","\n","            losses = []\n","\n","            # Calculate the loss for each task\n","            for logit in logits:\n","                losses.append(loss_fn(logit, labels.view(-1, 1)))\n","            \n","            # Sum the losses to get the total loss\n","            total_loss = sum(losses)\n","            \n","            total_loss.backward()\n","            optimizer.step()\n","            \n","            if batch_idx % 100 == 0:\n","                print(f\"Epoch: {epoch+1}/{num_epochs}, Subject: P{i+1:03d}, Batch: {batch_idx}, Loss: {total_loss.item()}\")\n","\n","        del df\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":0}
