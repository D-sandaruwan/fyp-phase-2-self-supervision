{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":652,"status":"ok","timestamp":1681452714221,"user":{"displayName":"Darshana Sandaruwan","userId":"08485959791759341660"},"user_tz":-330},"id":"DDHVRR0L8T6s"},"outputs":[],"source":["import pickle\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import os\n","\n","from torch.utils.data import TensorDataset, DataLoader\n","from sklearn.metrics import accuracy_score, f1_score, r2_score\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1681452718315,"user":{"displayName":"Darshana Sandaruwan","userId":"08485959791759341660"},"user_tz":-330},"id":"KQY5OHG6un7W"},"outputs":[],"source":["class Create1DConvCoreModel(nn.Module):\n","    def __init__(self, input_shape):\n","        super(Create1DConvCoreModel, self).__init__()\n","\n","        self.conv1d_1 = nn.Conv1d(in_channels=input_shape[1], out_channels=32, kernel_size=24)\n","        self.relu_1 = nn.ReLU()\n","        self.conv1d_2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=16)\n","        self.relu_2 = nn.ReLU()\n","        self.conv1d_3 = nn.Conv1d(in_channels=64, out_channels=96, kernel_size=8)\n","        self.relu_3 = nn.ReLU()\n","        self.maxpool_1d = nn.MaxPool1d(kernel_size=20)\n","        self.linear_1 = nn.Linear(in_features=7*96, out_features=256)\n","        self.relu_4 = nn.ReLU()\n","        self.linear_2 = nn.Linear(in_features=256, out_features=5)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, x):\n","        x = self.conv1d_1(x)\n","        x = self.relu_1(x)\n","        x = self.conv1d_2(x)\n","        x = self.relu_2(x)\n","        x = self.conv1d_3(x)\n","        x = self.relu_3(x)\n","        x = self.maxpool_1d(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.linear_1(x)\n","        x = self.relu_4(x)\n","        x = self.linear_2(x)\n","        x = self.softmax(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11319,"status":"ok","timestamp":1681453375734,"user":{"displayName":"Darshana Sandaruwan","userId":"08485959791759341660"},"user_tz":-330},"id":"nODOU_mwQI4z","outputId":"de54b82d-da3c-4c9c-a73e-9446126d9a49"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n"]}],"source":["data_dir = '/media/darshana/Software/dataset'\n","input_shape = (200, 3)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":91465,"status":"ok","timestamp":1681453467169,"user":{"displayName":"Darshana Sandaruwan","userId":"08485959791759341660"},"user_tz":-330},"id":"X8oJw7SQPQ8i","outputId":"ae372164-864a-47a5-814d-a000948636d5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded data for P001.obj\n","Encoded labels for P001.obj\n","Subject: P001.obj; Turn: 0; Loss: 1.6091\n","Subject: P001.obj; Turn: 1; Loss: 1.4335\n","Subject: P001.obj; Turn: 2; Loss: 1.4739\n","Subject: P001.obj; Turn: 3; Loss: 1.4429\n","Subject: P001.obj; Turn: 4; Loss: 1.4499\n","Subject: P001.obj; Turn: 5; Loss: 1.4346\n","Subject: P001.obj; Turn: 6; Loss: 1.4562\n","Subject: P001.obj; Turn: 7; Loss: 1.4281\n","Subject: P001.obj; Turn: 8; Loss: 1.4349\n","Subject: P001.obj; Turn: 9; Loss: 1.4588\n","Subject: P001.obj; Turn: 10; Loss: 1.4411\n","Subject: P001.obj; Turn: 11; Loss: 1.4690\n","Subject: P001.obj; Turn: 12; Loss: 1.4413\n","Subject: P001.obj; Turn: 13; Loss: 1.4289\n","Subject: P001.obj; Turn: 14; Loss: 1.4283\n","Subject: P001.obj; Turn: 15; Loss: 1.4495\n","Subject: P001.obj; Turn: 16; Loss: 1.4402\n","Subject: P001.obj; Turn: 17; Loss: 1.4650\n","Subject: P001.obj; Turn: 18; Loss: 1.4669\n","Subject: P001.obj; Turn: 19; Loss: 1.4613\n","Loaded data for P002.obj\n","Encoded labels for P002.obj\n","Subject: P002.obj; Turn: 0; Loss: 1.6802\n","Subject: P002.obj; Turn: 1; Loss: 1.4678\n","Subject: P002.obj; Turn: 2; Loss: 1.4970\n","Subject: P002.obj; Turn: 3; Loss: 1.4655\n","Subject: P002.obj; Turn: 4; Loss: 1.4487\n","Subject: P002.obj; Turn: 5; Loss: 1.4503\n","Subject: P002.obj; Turn: 6; Loss: 1.4541\n","Subject: P002.obj; Turn: 7; Loss: 1.4381\n","Subject: P002.obj; Turn: 8; Loss: 1.4524\n","Subject: P002.obj; Turn: 9; Loss: 1.4417\n","Subject: P002.obj; Turn: 10; Loss: 1.5131\n","Subject: P002.obj; Turn: 11; Loss: 1.4560\n","Subject: P002.obj; Turn: 12; Loss: 1.4386\n","Subject: P002.obj; Turn: 13; Loss: 1.4680\n","Subject: P002.obj; Turn: 14; Loss: 1.4350\n","Subject: P002.obj; Turn: 15; Loss: 1.4359\n","Subject: P002.obj; Turn: 16; Loss: 1.4373\n","Subject: P002.obj; Turn: 17; Loss: 1.4398\n","Loaded data for P003.obj\n","Encoded labels for P003.obj\n","Subject: P003.obj; Turn: 0; Loss: 1.7114\n","Subject: P003.obj; Turn: 1; Loss: 1.4657\n","Subject: P003.obj; Turn: 2; Loss: 1.4377\n","Subject: P003.obj; Turn: 3; Loss: 1.4559\n","Subject: P003.obj; Turn: 4; Loss: 1.4557\n","Subject: P003.obj; Turn: 5; Loss: 1.4593\n","Subject: P003.obj; Turn: 6; Loss: 1.4931\n","Subject: P003.obj; Turn: 7; Loss: 1.4674\n","Subject: P003.obj; Turn: 8; Loss: 1.4076\n","Subject: P003.obj; Turn: 9; Loss: 1.4264\n","Subject: P003.obj; Turn: 10; Loss: 1.4520\n","Subject: P003.obj; Turn: 11; Loss: 1.4624\n","Subject: P003.obj; Turn: 12; Loss: 1.4575\n","Subject: P003.obj; Turn: 13; Loss: 1.4672\n","Subject: P003.obj; Turn: 14; Loss: 1.4259\n","Subject: P003.obj; Turn: 15; Loss: 1.4514\n","Subject: P003.obj; Turn: 16; Loss: 1.4525\n","Subject: P003.obj; Turn: 17; Loss: 1.4385\n","Subject: P003.obj; Turn: 18; Loss: 1.4624\n","Loaded data for P004.obj\n","Encoded labels for P004.obj\n","Subject: P004.obj; Turn: 0; Loss: 1.4618\n","Subject: P004.obj; Turn: 1; Loss: 1.5040\n","Subject: P004.obj; Turn: 2; Loss: 1.4474\n","Subject: P004.obj; Turn: 3; Loss: 1.4741\n","Subject: P004.obj; Turn: 4; Loss: 1.4707\n","Subject: P004.obj; Turn: 5; Loss: 1.4575\n","Subject: P004.obj; Turn: 6; Loss: 1.4387\n","Subject: P004.obj; Turn: 7; Loss: 1.4453\n","Subject: P004.obj; Turn: 8; Loss: 1.4290\n","Subject: P004.obj; Turn: 9; Loss: 1.4548\n","Subject: P004.obj; Turn: 10; Loss: 1.4514\n","Subject: P004.obj; Turn: 11; Loss: 1.4878\n","Subject: P004.obj; Turn: 12; Loss: 1.4803\n","Subject: P004.obj; Turn: 13; Loss: 1.4835\n","Subject: P004.obj; Turn: 14; Loss: 1.4755\n","Subject: P004.obj; Turn: 15; Loss: 1.4334\n","Loaded data for P005.obj\n","Encoded labels for P005.obj\n","Subject: P005.obj; Turn: 0; Loss: 1.5553\n","Subject: P005.obj; Turn: 1; Loss: 1.5163\n","Subject: P005.obj; Turn: 2; Loss: 1.4879\n","Subject: P005.obj; Turn: 3; Loss: 1.5003\n","Subject: P005.obj; Turn: 4; Loss: 1.4852\n","Subject: P005.obj; Turn: 5; Loss: 1.4794\n","Subject: P005.obj; Turn: 6; Loss: 1.4585\n","Subject: P005.obj; Turn: 7; Loss: 1.4784\n","Subject: P005.obj; Turn: 8; Loss: 1.4733\n","Subject: P005.obj; Turn: 9; Loss: 1.4550\n","Subject: P005.obj; Turn: 10; Loss: 1.5039\n","Subject: P005.obj; Turn: 11; Loss: 1.4738\n","Subject: P005.obj; Turn: 12; Loss: 1.4890\n","Subject: P005.obj; Turn: 13; Loss: 1.4790\n","Subject: P005.obj; Turn: 14; Loss: 1.5005\n","Subject: P005.obj; Turn: 15; Loss: 1.4685\n","Subject: P005.obj; Turn: 16; Loss: 1.4820\n","Subject: P005.obj; Turn: 17; Loss: 1.4647\n","Subject: P005.obj; Turn: 18; Loss: 1.4680\n","Subject: P005.obj; Turn: 19; Loss: 1.4634\n","Loaded data for P006.obj\n","Encoded labels for P006.obj\n","Subject: P006.obj; Turn: 0; Loss: 1.4698\n","Subject: P006.obj; Turn: 1; Loss: 1.5037\n","Subject: P006.obj; Turn: 2; Loss: 1.4742\n","Subject: P006.obj; Turn: 3; Loss: 1.4544\n","Subject: P006.obj; Turn: 4; Loss: 1.4366\n","Subject: P006.obj; Turn: 5; Loss: 1.4456\n","Subject: P006.obj; Turn: 6; Loss: 1.4923\n","Subject: P006.obj; Turn: 7; Loss: 1.4645\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 39\u001b[0m\n\u001b[1;32m     35\u001b[0m dataloader \u001b[39m=\u001b[39m DataLoader(dataset, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     37\u001b[0m \u001b[39mfor\u001b[39;00m i, (x, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[1;32m     38\u001b[0m     \u001b[39m# Move x and y to the GPU device\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     40\u001b[0m     y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     41\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtranspose(x, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["label_encoder = LabelEncoder()\n","\n","# Create an instance of the model and move it to the GPU device\n","model = Create1DConvCoreModel(input_shape).to(device)\n","\n","num_epochs = 1\n","batch_size = 256\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# List of subjects to be used for training\n","subjects_train = [f'P{str(i).zfill(3)}.obj' for i in range(1, 8)]\n","\n","# Train the model incrementally using each subject's data\n","for subject_idx, subject in enumerate(subjects_train):\n","    with open(os.path.join(data_dir, subject), 'rb') as f:\n","        data = pickle.load(f)\n","    print(f'Loaded data for {subject}')\n","\n","    data['y'] = label_encoder.fit_transform(data['y'])\n","    print(f'Encoded labels for {subject}')\n","\n","    # Load the saved model if this is not the first subject\n","    if subject_idx > 0:\n","        model.load_state_dict(torch.load(f'model_after_{subjects_train[subject_idx - 1][:-4]}.pth'))\n","\n","    # Train the model using the data from the pickle file\n","    # Assuming data['X'] and data['y'] contain the input data and labels\n","    for epoch in range(num_epochs):\n","\n","        # Create a DataLoader to iterate over the data in batches\n","        dataset = TensorDataset(torch.tensor(data['X'], dtype=torch.float32), \n","                                torch.tensor(data['y'], dtype=torch.long))\n","        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","        for i, (x, y) in enumerate(dataloader):\n","            # Move x and y to the GPU device\n","            x = x.to(device)\n","            y = y.to(device)\n","            x = torch.transpose(x, 1, 2)\n","\n","            # Run the model forward\n","            y_pred = model(x)\n","\n","            # Compute the loss\n","            loss = criterion(y_pred, y)\n","\n","            # Compute the gradients and update the model\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            if i%100 == 0:\n","                print (f'Subject: {subject}; Turn: {i//100}; Loss: {loss.item():.4f}')\n","\n","    # Save the model after training with the current subject's data\n","    torch.save(model.state_dict(), f'model_after_{subject[:-4]}.pth')\n","\n","    # Clear memory\n","    del data\n","    torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":415},"executionInfo":{"elapsed":1161,"status":"error","timestamp":1681453548685,"user":{"displayName":"Darshana Sandaruwan","userId":"08485959791759341660"},"user_tz":-330},"id":"sWaHpriyU1Gd","outputId":"d2270cdb-fad5-4cb0-85a5-3f88b7ce0030"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test accuracy:\n","38.3515525718734\n"]}],"source":["# List of subjects to be used for testing\n","subjects_test = [f'P{str(i).zfill(3)}.obj' for i in range(8, 11)]\n","\n","# Test the model using each subject's data\n","for subject in subjects_test:\n","    with open(os.path.join(data_dir, subject), 'rb') as f:\n","        data = pickle.load(f)\n","\n","    data['y'] = label_encoder.fit_transform(data['y'])\n","\n","    # Create a DataLoader to iterate over the test data in batches\n","    test_dataset = TensorDataset(torch.tensor(data['X'], dtype=torch.float32), \n","                                torch.tensor(data['y'], dtype=torch.long))\n","    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    correct_t = 0\n","    total_t = 0\n","\n","    # Iterate over the test data in batches\n","    with torch.no_grad():\n","        for x, y in test_dataloader:\n","            # Move x to the GPU device and transpose it\n","            x = x.to(device)\n","            x = torch.transpose(x, 1, 2)\n","            \n","            # Run the model forward\n","            test_outputs = model(x)\n","            \n","            # Compute the predicted and actual labels\n","            _, predictedt = torch.max(test_outputs, 1)\n","            actual = y\n","            \n","            # Update the correct and total counts\n","            correct_t += (predictedt.cpu() == actual).sum().item()\n","            total_t += y.size(0)\n","\n","    # Calculate and print test accuracy\n","    test_accuracy = correct_t / total_t * 100\n","    print('Test accuracy:')\n","    print(test_accuracy)\n","\n","    # Clear memory\n","    del data\n","    torch.cuda.empty_cache()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # Define the path to the directory containing the pickle files\n","# data_dir = '/content/drive/Shareddrives/GitRepos/fyp-phase-2/preprocessing'\n","\n","# # Define the input shape of the model\n","# input_shape = (200, 3)\n","\n","# # Check if GPU is available\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# print(f\"Using device: {device}\")\n","\n","# # Loop over the pickle files\n","# for user_id in range(1, 11):\n","#     # Load the pickle file for the current user\n","#     with open(data_dir + f'/P{user_id:03d}.obj', 'rb') as f:\n","#         data = pickle.load(f)\n","\n","#     # Create an instance of the model and move it to the GPU device\n","#     model = Create1DConvCoreModel(input_shape).to(device)\n","\n","#     num_epochs = 10\n","\n","#     criterion = nn.CrossEntropyLoss()\n","#     optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","#     label_encoder = LabelEncoder()\n","#     data['y'] = label_encoder.fit_transform(data['y'])\n","\n","#     # Train the model using the data from the pickle file\n","#     # Assuming data['X'] and data['y'] contain the input data and labels\n","#     for epoch in range(num_epochs):\n","#         epoch_loss = 0\n","#         epoch_acc = 0\n","#         epoch_f1 = 0\n","#         epoch_r2 = 0\n","\n","#         for i, (x, y) in enumerate(zip(data['X'], data['y'])):\n","#             # Convert x and y to tensors and move them to the GPU device\n","#             x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(0).to(device)\n","#             y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(0).to(device)\n","\n","#             print(x_tensor[1])\n","\n","#             # Run the model forward\n","#             y_pred = model(x_tensor)\n","\n","#             # Compute the loss\n","#             loss = criterion(y_pred, y_tensor)\n","\n","#             # Compute the gradients and update the model\n","#             optimizer.zero_grad()\n","#             loss.backward()\n","#             optimizer.step()\n","\n","#             # Compute the accuracy, F1 score, and R2 score for the current batch\n","#             y_pred = torch.argmax(y_pred, dim=1)\n","#             acc = accuracy_score(y_tensor.cpu(), y_pred.cpu())\n","#             f1 = f1_score(y_tensor.cpu(), y_pred.cpu(), average='weighted')\n","#             r2 = r2_score(y_tensor.cpu(), y_pred.cpu())\n","\n","#             # Accumulate the metrics for the current epoch\n","#             epoch_loss += loss.item()\n","#             epoch_acc += acc\n","#             epoch_f1 += f1\n","#             epoch_r2 += r2\n","\n","#         # Compute the average metrics for the epoch\n","#         num_batches = len(data['X'])\n","#         epoch_loss /= num_batches\n","#         epoch_acc /= num_batches\n","#         epoch_f1 /= num_batches\n","#         epoch_r2 /= num_batches\n","\n","#         # Print the metrics for the epoch\n","#         print(f'User {user_id}, Epoch {epoch+1}:')\n","#         print(f'  Loss: {epoch_loss:.4f}')\n","#         print(f'  Accuracy: {epoch_acc:.4f}')\n","#         print(f'  F1 Score: {epoch_f1:.4f}')\n","#         print(f'  R2 Score: {epoch_r2:.4f}')\n","\n","#     # Delete the data object to free up memory\n","#     del data"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":0}
